---
title: "P8106: Data Science II, Homework #2"
author: 'Zachary Katz (UNI: zak2132)'
date: "3/8/2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(caret)
library(GGally)
library(gridExtra)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Set-Up and Data Preprocessing

```{r}
set.seed(2132)

# Load data, clean column names, eliminate rows containing NA entries
data = read_csv("./Data/College.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  relocate("outstate", .after = "grad_rate") %>% 
  select(-college)

# Partition data into training/test sets
indexTrain = createDataPartition(y = data$outstate,
                                 p = 0.8,
                                 list = FALSE)

training_df = data[indexTrain, ]

testing_df = data[-indexTrain, ]

# Create matrices for future analysis

# Training data
x_train = model.matrix(outstate~.,training_df)[, -1]
y_train = training_df$outstate

# Testing data
x_test <- model.matrix(outstate~.,testing_df)[, -1]
y_test <- testing_df$outstate
```

## Part (a): Exploratory Data Analysis

```{r}
# Summary statistics
summary(training_df)
skimr::skim(training_df)
```

In total, our training data set has 453 observations on 17 variables, with no data incompleteness. Of the 17 variables, our single response (outcome) variable is `outstate`, representing out of state tuition. Our other 16 variables are continuous, numeric variables representing a range of predictors, from annual applications to cost of room and board. Notably, we have excluded college name (`college`) as a predictor given its presumed irrelevance to any kind of predictive model. 

```{r}
# EDA scatterplots
# Set visual theme settings
theme1 = trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(.8, .1, .1, 1)
theme1$plot.line$lwd = 2
theme1$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

# All predictors are continuous; scatterplots most useful for data viz
featurePlot(x_train, y_train, plot = "scatter", labels = c("","Y"), type = c("p"))

# Pairwise relationships show numerous multicollinearities
ggpairs(training_df)

```

At first glance, the clearest linear relationships with `outstate` seem to be with predictors `perc_alumni`, `grad_rate`, `ph_d`, `terminal`, `top25perc`, `room_board`, and `top10perc`. In addition, we observe numerous multicollinearities with correlation greater than 0.90, including `apps` and `enroll`, `enroll` and `accept`, `top25perc` and `top10perc`, and several others.

## Part (b): Smoothing Spline Models

```{r}
set.seed(2132)
# Fit smoothing spline using `terminal` as only predictor of `outstate`
# By default, uses generalized cross-validation to select lambda value (smoothing parameter)
fit_smooth_spline = smooth.spline(training_df$terminal, training_df$outstate)

# Optimal degrees of freedom based on cross-validation
fit_smooth_spline$df

# Prediction on grid of terminal values
# Using min and max values from training and testing data
terminal_grid <- seq(from = 24, to = 100, by = 1)

pred_smooth_spline_grid = predict(fit_smooth_spline, x = terminal_grid)

pred_smooth_spline_grid_df = data.frame(predicted = pred_smooth_spline_grid$y,
                                        terminal = terminal_grid)

p = ggplot(data = testing_df, aes(x = terminal, y = outstate)) +
     geom_point(color = rgb(.2, .4, .2, .5))

p + geom_line(aes(x = terminal, y = predicted), data = pred_smooth_spline_grid_df,
              color = rgb(.8, .1, .1, 1)) + theme_bw()

# Prediction on test data
pred_smooth_spline_testing = predict(fit_smooth_spline, x = testing_df$terminal)

pred_smooth_spline_testing_df = data.frame(predicted = pred_smooth_spline_testing$y,
                                           terminal = testing_df$terminal)

p + geom_line(aes(x = terminal, y = predicted), data = pred_smooth_spline_testing_df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()
```

Here, we fit a smoothing spline model using `terminal` as the only predictor of `outstate` for the optimal degrees of freedom obtained by generalized cross-validation, which is about 4.4. However, we'd also like to understand how the model fit changes with a range of degrees of freedom, which we plot below:


```{r}
# Try a range of degrees of freedom in function
spline_fn = function(degree){
  
  spline_fit = smooth.spline(training_df$terminal, training_df$outstate, df = degree)
  
  spline_pred = predict(spline_fit, x = terminal_grid)
  
  spline_df = data.frame(predicted = spline_pred$y,
                         terminal = terminal_grid,
                         df = degree)
  
}

# Run spline function for degrees of freedom 2 through 15
datalist = list()
for (i in 2:15) {
  datalist[[i]] = spline_fn(i)
}
all_data = do.call(rbind, datalist) %>% 
  as.data.frame()

# Plot results for range of degrees of freedom
# Red line represents optimal df from base R function
p + 
  geom_line(aes(x = terminal, y = predicted, group = df, color = df), data = all_data) + 
  geom_line(aes(x = terminal, y = predicted), data = pred_smooth_spline_testing_df,
          color = rgb(.8, .1, .1, 1))
```

Overlaying the model with different degrees of freedom (ranging from 2 to 15), we see that with fewer than 4 degrees of freedom, our model fit is more linear. As we increase the degrees of freedom much beyond 4, there is more overfitting; our models start to "wiggle" more. We can observe that our optimized model with about 4.4 degrees of freedom optimally fits the data.

## Part (c): Generalized Additive Model

```{r}

set.seed(2132)

ctrl1 = trainControl(method = "cv", number = 10)

# Check whether any predictors take on fewer than 10 values
# None do, so we can use the caret function, which at times results in loss of flexibility when we have predictors that take on <= 10 values
sapply(x_train %>% as.data.frame(), n_distinct)

# Run GAM in caret
# Use automatic feature selection (Cp method)
gam_fit = train(x_train, y_train,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp",
                                      select = c(TRUE, FALSE)),
                trControl = ctrl1)

# Parameters that fit the best model
gam_fit$bestTune
gam_fit$finalModel

# Summary of final model
summary(gam_fit)

# Plot of final model
par(mar=c(1,1,1,1))
par(mfrow = c(4, 4))
plot(gam_fit$finalModel, residuals = TRUE, all.terms = TRUE, shade = TRUE, shade.col = 2)

# Calculate training MSE of optimal model
gam_train_MSE = mean((y_train - predict(gam_fit))^2)
gam_train_MSE

gam_train_RMSE = sqrt(gam_train_MSE)
gam_train_RMSE

# Calculate test MSE of optimal model
test_predictions = predict(gam_fit, x_test)

gam_test_MSE = mean((y_test - test_predictions)^2)
gam_test_MSE

gam_test_RMSE = sqrt(gam_test_MSE)
gam_test_RMSE
```

We may choose to fit our GAM using either MCGV or the `caret` package. Notably, the latter may result in loss of flexibility since it automatically precludes the possibility of nonlinear transformations for predictors that take fewer than 10 unique values. However, in this case, all of our predictors take more than 10 unique values, and so we do not expect loss of flexibility by using `caret`.

Using all of our predictors, our best model attains an MSE of 2251375 (RMSE 1500.5) when we apply our model to the training data and an MSE of 3364712 (RMSE 1834.3) when we apply it to the hold-out test data from our original partitioning. In our output summary, the "parametric coefficients" refers to the linear terms of the model, which in this case only includes the intercept. Coefficients are not printed for our smooth terms because each smooth term has several coefficients corresponding to different basis functions. Instead, we have effective degrees of freedom, which represent the complexity of the smooth function. `terminal`, `top10perc`, `top25perc`, `personal`, `p_undergrad`, and `enroll` all have one effective degree of freedom, corresponding to a straight line; our graphs confirm these linear relationships. Those with effective degrees of freedom around two, such as `perc_alumni` and `books`, are quadratically incorporated, whereas those with effective degrees of freedom around three, such as `grad_rate` and `accept`, are cubically incorporated, and so on. In our model, `perc_alumni`, `enroll`, `room_board`, `f_undergrad`, and `expend` are our most significant smooth terms.


## Part (d): Multivariate Adaptive Regression Spline Model

```{r}
set.seed(2132)

ctrl1 = trainControl(method = "cv", number = 10)

# Grid of tuning parameters
mars_grid = expand.grid(degree = 1:3, 
                         nprune = 2:25)

# Fit MARS model
mars_fit = train(x_train, y_train,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

# Choose optimal model with 1 degree and 18 hinge functions to minimize RMSE
# Note that n terms includes the intercept
ggplot(mars_fit)
mars_fit$bestTune

# Model summary of best fit
summary(mars_fit$finalModel)

# Coefficients (betas) in front of each hinge function
# Note that you can have more than 1 hinge function per predictor
# In this case, we use 10 of 16 predictors
coef(mars_fit$finalModel) 

# Report train MSE
mars_train_MSE = mean((y_train - predict(mars_fit))^2)
mars_train_MSE

mars_train_RMSE = sqrt(mars_train_MSE)
mars_train_RMSE

# Report test MSE
test_predictions_mars = predict(mars_fit, x_test)

mars_test_MSE = mean((y_test - test_predictions_mars)^2)
mars_test_MSE

mars_test_RMSE = sqrt(mars_test_MSE)
mars_test_RMSE
```

Here, we train a MARS model using all predictors, finding that the optimal model attains an MSE of 2421360 (RMSE 1556.1) when we apply our model to the training data and an MSE of 3460709 (RMSE 1860.3) when we apply it to the hold-out test data from our original partitioning. The final model minimizes RMSE using one product degree (maximum degree of interactions, i.e. our final model is only an additive model) and 18 maximum terms, including intercept, from our `nprune` tuning parameter. 15 of 22 terms were used from 10 of the 16 original predictors. The 15 terms in our model include hinge functions and intercept. For example, looking at `apps`, we know that a knot occurs at 3767, and looking at `accept`, a knot occurs at 2109. Note that these include boundary knots. The most important predictors for for our outcome appear to be `expend`, `grad_rate`, `accept`, and `enroll`.

```{r}
# Present partial dependence plot of arbitrary predictor in final model

partial_one_pred = pdp::partial(mars_fit, pred.var = c("expend"),
                  grid.resolution = 10) %>% 
  autoplot(smooth = TRUE, ylab = expression(f(expend))) + 
  theme_light()

# Try with two predictors at same time, for fun

partial_two_pred = pdp::partial(mars_fit, pred.var =  c("expend", "enroll"),
                  grid.resolution = 10) %>% 
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE,
                   screen = list(z = 20, x = -60))

grid.arrange(partial_one_pred, partial_two_pred, ncol = 2)
```

Above, we present two partial dependency plots: the first, for `expend` only, and the second, for both `expend` and `enroll`. Looking at `expend`, for example, we find a single internal knot at 14773, which corresponds to the MARS model summary printed above. As a college exceeds 14773 on the `expend` metric, each additional unit of `expend` sees a marginal decrease in `outstate` compared to colleges with less than 14773 in `expend`. The interaction plot on the right illustrates the stronger effect `expend` and `enroll` might have when combined on `outstate`, for instance.

## Part (e): Selecting a Model

```{r}
resamp = resamples(list(gam_model = gam_fit,
                        mars_model = mars_fit))

summary(resamp)

bwplot(resamp, metric = "RMSE")
```

```{r}
# Alternative method

resamp_caret = caret::resamples(list(gam_model = gam_fit,
                        mars_model = mars_fit))

summary(resamp_caret)

bwplot(resamp_caret, metric = "RMSE")
```

In this example, we prefer the MARS model over a linear model when predicting out-of-state tuition because we minimize RMSE with our fitted MARS model, and thus we see a stronger model fit.
